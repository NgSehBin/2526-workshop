{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8831896,"sourceType":"datasetVersion","datasetId":5314260},{"sourceId":521980,"sourceType":"modelInstanceVersion","modelInstanceId":410137,"modelId":279036}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Environment and GPU Configuration \n\nThis cell prepares the runtime for fine-tuning.\nWe set the Keras backend to TensorFlow, specify which GPUs should be visible, enable dynamic GPU memory growth to avoid full memory reservation, and suppress unnecessary TensorFlow logs.\n\nAfter applying these settings, we import TensorFlow, check how many GPUs are available, and enable memory-growth on each one. This ensures stable GPU usage when loading and training the Gemma 3 model.","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# Import TensorFlow FIRST to lock in GPU\nimport tensorflow as tf\ngpus = tf.config.list_physical_devices('GPU')\nprint(f\"Initial GPU check: {len(gpus)} GPUs\")\n\nif gpus:\n    for gpu in gpus:\n        tf.config.experimental.set_memory_growth(gpu, True)\n    print(\"✓ GPUs configured\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:48:04.812914Z","iopub.execute_input":"2025-12-07T11:48:04.81314Z","iopub.status.idle":"2025-12-07T11:48:27.842182Z","shell.execute_reply.started":"2025-12-07T11:48:04.813116Z","shell.execute_reply":"2025-12-07T11:48:27.841484Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Upgrade KerasNLP to the Latest Version\n\nThis cell installs the latest version of KerasNLP, which includes full support for the Gemma 3 model family.\nWe run a simple pip upgrade command, and then print a confirmation.\nDo not restart the runtime after installing, because TensorFlow and the GPU setup from the previous cell would reset.","metadata":{}},{"cell_type":"code","source":"# Upgrade to latest KerasNLP for Gemma3 support\n!pip install -q --upgrade keras-nlp\n!\n\nprint(\"✓ KerasNLP upgraded to latest - continue to next cell (do NOT restart)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:48:27.843544Z","iopub.execute_input":"2025-12-07T11:48:27.844005Z","iopub.status.idle":"2025-12-07T11:48:35.01731Z","shell.execute_reply.started":"2025-12-07T11:48:27.843986Z","shell.execute_reply":"2025-12-07T11:48:35.016567Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Verify Installation and Environment\n\nThis cell performs several checks before we start fine-tuning:\n\n\n1. Imports required libraries: keras, keras_nlp, TopKSampler, time, csv, and logging.\n2. Suppresses verbose logs from sentencepiece.\n3. Prints the current versions of Keras and KerasNLP.\n4. Re-checks that GPUs are still available.\n5. Verifies that the Gemma3CausalLM model is present in KerasNLP.\n\n\nThis ensures the environment is correctly set up and ready for model fine-tuning.","metadata":{}},{"cell_type":"code","source":"import keras\nimport keras_nlp\nfrom keras_nlp.samplers import TopKSampler\nfrom time import time\nimport csv\nimport logging\n\n# Suppress messages\nlogging.getLogger(\"sentencepiece\").setLevel(logging.ERROR)\n\nprint(\"=\"*60)\nprint(\"KerasNLP version:\", keras_nlp.__version__)\nprint(\"Keras version:\", keras.__version__)\n\n# Re-verify GPU\nimport tensorflow as tf\ngpus = tf.config.list_physical_devices('GPU')\nprint(f\"Num GPUs: {len(gpus)}\")\n\nif gpus:\n    print(\"✓✓✓ GPU STILL DETECTED! ✓✓✓\")\nelse:\n    print(\"⚠️ GPU lost\")\n    \n# Check Gemma3\nif hasattr(keras_nlp.models, 'Gemma3CausalLM'):\n    print(\"✓ Gemma3CausalLM available!\")\nelse:\n    print(\"✗ Gemma3CausalLM NOT available\")\n    print(f\"Available: {[x for x in dir(keras_nlp.models) if 'Gemma' in x]}\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:48:35.018187Z","iopub.execute_input":"2025-12-07T11:48:35.018398Z","iopub.status.idle":"2025-12-07T11:48:36.913853Z","shell.execute_reply.started":"2025-12-07T11:48:35.018377Z","shell.execute_reply":"2025-12-07T11:48:36.91318Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Gemma3 270M Model\n\nThis cell loads the Gemma3 270M causal language model using KerasNLP’s from_preset method.\nWe use the Kaggle-hosted preset to get the pre-trained weights and configuration.\nOnce loaded, the model is ready for fine-tuning.","metadata":{}},{"cell_type":"code","source":"# Load the model\n# We load the model gemma_3_270M using keras_nlp.\nprint(\"Loading Gemma3 270M model...\")\ngemma_lm = keras_nlp.models.Gemma3CausalLM.from_preset(\"kaggle://keras/gemma3/keras/gemma3_270m/3\")\nprint(\"✓ Model loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:48:36.914693Z","iopub.execute_input":"2025-12-07T11:48:36.914952Z","iopub.status.idle":"2025-12-07T11:48:48.784169Z","shell.execute_reply.started":"2025-12-07T11:48:36.914919Z","shell.execute_reply":"2025-12-07T11:48:48.783459Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inspect Model Architecture\n\nThis cell displays a summary of the Gemma3 270M model, including:\n\nLayer types\n\nOutput shapes\n\nNumber of parameters\n\nIt helps us understand the model structure and verify that it loaded correctly.","metadata":{}},{"cell_type":"code","source":"gemma_lm.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:48:48.784923Z","iopub.execute_input":"2025-12-07T11:48:48.785176Z","iopub.status.idle":"2025-12-07T11:48:48.809103Z","shell.execute_reply.started":"2025-12-07T11:48:48.785157Z","shell.execute_reply":"2025-12-07T11:48:48.808355Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Define Training Configuration\n\nThis cell sets up a simple configuration class CFG that contains key hyperparameters for fine-tuning:\n\n\n* max_length: Maximum sequence length for input text.\n* data_size: Number of training examples to use.\n* lora_rank: Rank for LoRA (Low-Rank Adaptation) fine-tuning.\n* epochs: Number of training epochs.\n* batch_size: Number of samples per training batch.\n\n\nAn instance cfg is created so these parameters can be easily accessed throughout the notebook.","metadata":{}},{"cell_type":"code","source":"class CFG:\n \n    max_length = 128\n    data_size = 2560\n    lora_rank = 16\n    epochs = 40\n    batch_size = 2\n\ncfg = CFG()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:48:48.80978Z","iopub.execute_input":"2025-12-07T11:48:48.810658Z","iopub.status.idle":"2025-12-07T11:48:48.815091Z","shell.execute_reply.started":"2025-12-07T11:48:48.81063Z","shell.execute_reply":"2025-12-07T11:48:48.81459Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load and Prepare Dataset\n\nThis cell reads a CSV file containing medical question-answer pairs and converts it into a format suitable for fine-tuning.\n\nThe CSV has two columns: question and answer.\n\nEach row is transformed into a dictionary with keys prompts (from question) and responses (from answer).\n\nAll examples are collected in a list called data.","metadata":{}},{"cell_type":"code","source":"data = []\n\n# The CSV file contains two columns 'question' and 'answer'\nwith open(\"//kaggle/input/medquad/medquad.csv\", mode='r', encoding='utf-8') as file:\n    reader = csv.DictReader(file)\n    for row in reader:\n        # we replace with 'prompts' and 'responses'\n        data.append({\"prompts\": row['question'], 'responses': row['answer']})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:48:48.817137Z","iopub.execute_input":"2025-12-07T11:48:48.817436Z","iopub.status.idle":"2025-12-07T11:48:49.23186Z","shell.execute_reply.started":"2025-12-07T11:48:48.817419Z","shell.execute_reply":"2025-12-07T11:48:49.23123Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Data size: {len(data)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:48:49.232537Z","iopub.execute_input":"2025-12-07T11:48:49.232838Z","iopub.status.idle":"2025-12-07T11:48:49.237315Z","shell.execute_reply.started":"2025-12-07T11:48:49.232812Z","shell.execute_reply":"2025-12-07T11:48:49.236561Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Limit Dataset Size\n\nThis cell trims the dataset to the first cfg.data_size examples.\nThis allows faster training and easier experimentation while still using a representative subset of the data.","metadata":{}},{"cell_type":"code","source":"data = data[:cfg.data_size]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:48:49.238754Z","iopub.execute_input":"2025-12-07T11:48:49.23899Z","iopub.status.idle":"2025-12-07T11:48:49.257987Z","shell.execute_reply.started":"2025-12-07T11:48:49.238973Z","shell.execute_reply":"2025-12-07T11:48:49.257456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Data size: {len(data)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:48:49.258775Z","iopub.execute_input":"2025-12-07T11:48:49.258958Z","iopub.status.idle":"2025-12-07T11:48:49.274461Z","shell.execute_reply.started":"2025-12-07T11:48:49.258944Z","shell.execute_reply":"2025-12-07T11:48:49.273762Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Convert Data to TensorFlow Dataset\n\nThis cell converts the Python list data into a TensorFlow tf.data.Dataset, which is optimized for training.\n\nWe use a generator to yield each dictionary from data.\n\noutput_signature specifies the expected shape and type for each field: both prompts and responses are strings.\n\nThis allows TensorFlow to efficiently batch, shuffle, and prefetch the dataset for training.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\ndataset = tf.data.Dataset.from_generator(\n    lambda: (item for item in data),\n    output_signature={\n        \"prompts\": tf.TensorSpec(shape=(), dtype=tf.string),\n        \"responses\": tf.TensorSpec(shape=(), dtype=tf.string),\n    }\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:48:49.275259Z","iopub.execute_input":"2025-12-07T11:48:49.275569Z","iopub.status.idle":"2025-12-07T11:48:49.325191Z","shell.execute_reply.started":"2025-12-07T11:48:49.275551Z","shell.execute_reply":"2025-12-07T11:48:49.324622Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import display, Markdown\ndef colorize_text(text):\n    for word, color in zip([\"Category\", \"Question\", \"Answer\"], [\"blue\", \"red\", \"green\"]):\n        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:48:49.325956Z","iopub.execute_input":"2025-12-07T11:48:49.326221Z","iopub.status.idle":"2025-12-07T11:48:49.330533Z","shell.execute_reply.started":"2025-12-07T11:48:49.326182Z","shell.execute_reply":"2025-12-07T11:48:49.329759Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Display a Sample from the Dataset\n\nThis cell displays the 4th example (data[3]) from the dataset using the colorize_text_dict function.\nIt shows the prompts (question) in red and responses (answer) in green for easy visual inspection.","metadata":{}},{"cell_type":"code","source":"def colorize_text_dict(sample):\n    \"\"\"\n    sample: dict with keys 'prompts' and 'responses'\n    \"\"\"\n    colored_text = \"\"\n    colored_text += f\"**<font color='red'>Question:</font>** {sample['prompts']}\\n\\n\"\n    colored_text += f\"**<font color='green'>Answer:</font>** {sample['responses']}\\n\\n\"\n    return colored_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:48:49.331346Z","iopub.execute_input":"2025-12-07T11:48:49.33174Z","iopub.status.idle":"2025-12-07T11:48:49.347669Z","shell.execute_reply.started":"2025-12-07T11:48:49.331714Z","shell.execute_reply":"2025-12-07T11:48:49.34712Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(data[3])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:48:49.348184Z","iopub.execute_input":"2025-12-07T11:48:49.348363Z","iopub.status.idle":"2025-12-07T11:48:49.363551Z","shell.execute_reply.started":"2025-12-07T11:48:49.348349Z","shell.execute_reply":"2025-12-07T11:48:49.363022Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display(Markdown(colorize_text_dict(data[3])))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:48:49.364291Z","iopub.execute_input":"2025-12-07T11:48:49.364525Z","iopub.status.idle":"2025-12-07T11:48:49.381516Z","shell.execute_reply.started":"2025-12-07T11:48:49.364504Z","shell.execute_reply":"2025-12-07T11:48:49.380962Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generate a Sample Response\n\nThis cell demonstrates how the Gemma3 270M model generates text:\n\nWe create a prompt with a question (prompts) and an empty response (responses).\n\nThe model generates a response with gemma_lm.generate.\n\nWe format the generated answer and display it using colorize_text_dict for readability.","metadata":{}},{"cell_type":"code","source":"prompt = {\n    \"prompts\":\"What are the treatments for Glaucoma ?\",\n    \"responses\":\"\"}\nresponse = gemma_lm.generate(prompt, max_length=cfg.max_length)\n\nanswer = {\"prompts\": prompt[\"prompts\"][0], \"responses\": response[len(prompt[\"prompts\"][0]):]}\ndisplay(Markdown(colorize_text_dict(answer)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:48:49.382215Z","iopub.execute_input":"2025-12-07T11:48:49.382461Z","iopub.status.idle":"2025-12-07T11:49:08.645408Z","shell.execute_reply.started":"2025-12-07T11:48:49.382445Z","shell.execute_reply":"2025-12-07T11:49:08.644777Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Enable LoRA Fine-Tuning\n\nThis cell enables LoRA (Low-Rank Adaptation) on the Gemma3 model:\n\nLoRA allows parameter-efficient fine-tuning by only training low-rank matrices instead of the full model.\n\nWe set the LoRA rank to cfg.lora_rank as defined in our configuration.\n\nAfter enabling LoRA, we display the model summary to verify the changes.","metadata":{}},{"cell_type":"code","source":"# Enable LoRA for the model and set the LoRA rank to cfg.lora_rank.\ngemma_lm.backbone.enable_lora(rank=cfg.lora_rank)\ngemma_lm.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:49:08.646283Z","iopub.execute_input":"2025-12-07T11:49:08.646829Z","iopub.status.idle":"2025-12-07T11:49:08.797255Z","shell.execute_reply.started":"2025-12-07T11:49:08.646801Z","shell.execute_reply":"2025-12-07T11:49:08.79672Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fine-Tune Gemma3 on Medical QA Dataset\n\nThis cell performs the actual fine-tuning of the model:\n\n\n* Limits input sequences to cfg.max_length to control GPU memory usage.\n* Uses AdamW optimizer, common for transformer models, with weight decay.Excludes biases and layer norm parameters from weight decay.\n* Uses SparseCategoricalCrossentropy as the loss function and tracks accuracy.\n* Batches the dataset according to cfg.batch_size and trains for cfg.epochs epochs.\n* Training history is saved in the history variable for later analysis.\n","metadata":{}},{"cell_type":"code","source":"# Fine-tune on the Medical QA dataset.\n\n# Limit the input sequence length to 128 (to control memory usage).\ngemma_lm.preprocessor.sequence_length = cfg.max_length\n# Use AdamW (a common optimizer for transformer models).\noptimizer = keras.optimizers.AdamW(\n    learning_rate=5e-5,\n    weight_decay=0.01,\n)\n# Exclude layernorm and bias terms from decay.\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\nbatched_dataset = dataset.batch(cfg.batch_size)\n\nhistory = gemma_lm.fit(batched_dataset, epochs=cfg.epochs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:49:08.79818Z","iopub.execute_input":"2025-12-07T11:49:08.798473Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualize Training Performance\n\nThis cell plots the training loss and accuracy over epochs using Matplotlib:\n\n\n* loss tracks the model’s cross-entropy loss.\n* accuracy tracks the model’s Sparse Categorical Accuracy.\n* Separate plots are generated to visualize how the model improved during fine-tuning.\n","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nloss = history.history['loss']\naccuracy = history.history['sparse_categorical_accuracy']\n\n# Plot Loss\nplt.plot(loss, label='Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.legend()\nplt.show()\n\n# Plot Accuracy\nplt.plot(accuracy, label='Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generate Response After Fine-Tuning\n\nThis cell tests the fine-tuned Gemma3 model:\n\nWe create a prompt using a template with a question and empty answer.\n\nThe model generates a response using gemma_lm.generate.\n\nThe output is displayed using colorize_text for better readability in the notebook.","metadata":{}},{"cell_type":"code","source":"template = \"Question:\\n{question}\\n\\nAnswer:\\n{answer}\"\nprompt = template.format(\n    question=\"What are the complications of Paget's Disease of Bone ?\",\n    answer=\"\",\n)\nresponse = gemma_lm.generate(prompt, max_length=cfg.max_length)\ndisplay(Markdown(colorize_text(response)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt = template.format(\n    question=\"What are the treatments for Diabetes ?\",\n    answer=\"\",\n)\nresponse = gemma_lm.generate(prompt, max_length=cfg.max_length)\ndisplay(Markdown(colorize_text(response)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt = template.format(\n    question=\"What are the symptoms of Glaucoma ?\",\n    answer=\"\",\n)\nresponse = gemma_lm.generate(prompt, max_length=cfg.max_length)\ndisplay(Markdown(colorize_text(response)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt = template.format(\n    question=\"What are the treatments for Glaucoma ?\",\n    answer=\"\",\n)\nresponse = gemma_lm.generate(prompt, max_length=cfg.max_length)\ndisplay(Markdown(colorize_text(response)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt = template.format(\n    question=\"What causes High Blood Pressure ?\",\n    answer=\"\",\n)\nresponse = gemma_lm.generate(prompt, max_length=cfg.max_length)\ndisplay(Markdown(colorize_text(response)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}